---
title: "Initial Exploration & Cleaning"
author: "Tripp Bishop"
format: html
editor: visual
---

```{r setup}
#| echo: false
#| message: false
library(tidyverse)
library(janitor)
library(visdat)
library(naniar)
library(gt)
theme_set(theme_minimal())
source("scripts/helper-functions.R")
default_fill_colour <- "#1f63b4"
```

## Data Analyst Jobs Dataset

The Data Analyst Jobs dataset was produced by scraping *Glassdoor.com* data analyst job postings. The date that this was done is not known, but the covid-19 pandemic is referenced so a likely lower bound on the date is mid-2020.

In the initial phase of the analysis, it is important to import the data, take a quick look at the first few records, and then glimpse the dataset to get a better sense of its structure.

```{r}
#| message: false

df_jobs <- read_csv("data/DataAnalyst.csv")
head(df_jobs)
```

Right away we can see that we have a superfluous variable, ...1, and that at least two columns contain "-1". There is a good chance that "-1" is the NA placeholder for this dataset. This will need to be addressed after the initial exploration of the data is complete. There are a several variables that will need to be converted to factors, some nominal and some ordinal. There are also some variables with non-standard names. These should be renamed so that they are easier to work with in the code.

```{r}
glimpse(df_jobs)
```

Start by renaming the variables and removing the row id variabl, it won't be necessary.

```{r rename variables}
df_jobs <- df_jobs |> clean_names() |> select(-x1)
```

## Exploring potential factors variables

The `size`, `type_of_ownership`, `industry`, `sector`, and `revenue` variables are good candidates for conversion to factors. The first step is understanding how many distinct values each variable has.

### `size`

This field contains categorical data about the size of the company listing the job.

```{r}
df_jobs |> distinct(size)
```

There is a -1 which indicates missingness. Furthermore, the value of "Unknown" is not helpful, so it should be converted to `NA` as well to facilitate analysis.

### `industry`

This field contains categorical data about the industry of the company listing the job.

```{r}
df_jobs |> distinct(industry)
```

There is a -1 which indicates missingness. It will be converted to `NA` during the cleaning phase.

### `type_of_ownership`

This feature indicates the ownership model of the company listing the job.

```{r}
df_jobs |> distinct(type_of_ownership)
```

In addition to -1, `type_of_ownership` has a value of "Unknown". These will need to be cleaned up and converted to `NA` values.

### `sector`

`sector` is related to the `industry` feature but is more general.

```{r}
df_jobs |> distinct(sector)
```

In addition to -1, `sector` has a value of "Unknown". These will need to be cleaned up and converted to `NA` values.

### `revenue`

This is a categorical feature that indicates how much revenue the company listing the job has reported. The levels of this feature are not of uniform width.

```{r}
df_jobs |> distinct(revenue)
```

There is no -1 value in `revenue`, but the value "Unknown / Non-Applicable" is present. This will be converted to `NA` for reasons state earlier.

## Exploring remaining features

The remaining features will need to be examined for missingness, although they will not be converted to factors.

### `founded`

```{r}
df_jobs |> distinct(founded) |> arrange(founded)
```

`founded` is a numeric variable, but does have -1 as a value.

### `rating`

`rating` is a numeric variable that appears to have a valid range of 1 to 5.

```{r}
df_jobs |> distinct(rating) |> arrange(rating)
```

 The value -1 is present and will be converted to `NA`.

### `location`

This feature indicates the city and state where the job is located. It contains two pieces of data, the city and state, so in order to make the data tidy, this field will be broken up into two new fields, `city` and `state`.

```{r}
df_jobs |> distinct(location) |> arrange(location)
```

`location` does not contain any "-1" values.

### `headquarters`

This feature contains information about where the headquarters of the listing company is located. It will not be used in the analysis and will be dropped.

```{r}
df_jobs |> distinct(headquarters) |> arrange(headquarters)
```

### `salary_estimate`

This feature contain the salary range of the job listing reported in thousands of dollars.

```{r}
df_jobs |> distinct(salary_estimate) |> arrange(salary_estimate)
```

`salary_estimate` does contain "-1" as a value. Every value also ends with the string "(Glassdoor est.)". In addition to this, the field contains the range from low to high in the formation \$AK - \$BK, where A & B are 2-3 digit integers. This field should be cleaned and then split into `salary_lower_end` and `salary_upper_end` numeric variables.

### `company_name`

```{r}
df_jobs |> distinct(company_name) |> arrange(company_name)
```

The `company_name` feature has many values containing a newline character followed by what appears to be the rating value. This should be removed. There is a company with the name "1". This could be a mistake, but the feature will not factor into any of the analysis

## Addressing missing values

The `easy_apply` variable should be a logical field. Rather than having `NA` for FALSE values, simply set all "-1" values to FALSE and all others to TRUE. This will force the `easy_apply` to be a logical variable. Next, the -1 and "Unknown" values can also be mapped to `NA` to complete the task. We can use features of the `naniar` package to understand the missing values much better if they are all set to `NA` rather than a combination of `NA` and "Unknown".

```{r cleaning missing values}
df_jobs <- df_jobs |> 
  mutate(
    easy_apply = if_else(easy_apply == "-1", FALSE, TRUE),
  ) |> 
  replace_with_na(df_jobs, 
                  replace=list(
                    salary_estimate = "-1",
                    rating = -1,
                    headquarters = -1,
                    founded = -1,
                    size = c("-1", "Unknown"),
                    type_of_ownership = c("Unknown","-1"),
                    industry = "-1",
                    sector = "-1",
                    revenue = c("-1", "Unknown / Non-Applicable"),
                    competitors = "-1"
                  )
  )
```

## Cleaning variables

Now that missing values have been standardised, the other data issues that have been identified can be addressed. `company_name` will have the superfluous characters removed, several factors will be created, and the `salary_estimate` data will be extracted into two new variables: `salary_lower_end` and `salary_upper_end`.

The factors are quite wordy and this will impact the visualisation of data using these variables, so prior to creating factors, it will be good to simplify the values without obscuring their meaning.

```{r updating revenue and size categories}
df_jobs <- df_jobs |> 
  mutate(
    # drop the \n + rating information from the company name
    company_name = str_replace(company_name, "\\n.+$", ""),
    size = str_remove(size, "\\semployees"),
    revenue = str_remove(revenue, "\\s\\(USD\\)"),
    revenue = str_replace(revenue, "\\sbillion", "B"),
    revenue = str_replace(revenue, "\\smillion", "M")
  )
```

```{r }
df_jobs <- df_jobs |> 
  mutate(
    # make industry, sector, any type_of_ownership nominal factors
    industry = as_factor(industry),
    sector = as_factor(sector),
    type_of_ownership = as_factor(type_of_ownership),
    # make size and revenue ordinal factors
    size = fct_relevel(size, 
                       "1 to 50",
                       "51 to 200",
                       "201 to 500",
                       "501 to 1000",
                       "1001 to 5000",
                       "5001 to 10000",
                       "10000+"),
    revenue = fct_relevel(revenue, 
                       "Less than $1M",
                       "$1 to $5M",
                       "$5 to $10M",
                       "$10 to $25M",
                       "$25 to $50M",
                       "$50 to $100M",
                       "$100 to $500M",
                       "$500M to $1B",
                       "$1 to $2B",
                       "$2 to $5B",
                       "$5 to $10B",
                       "$10+B")
  )
```

## Explore missing values

To begin, the `vis_dat` function from the `visdat` package is used to get a quick look at where missing values are and roughly how frequently they occur in each variable.

```{r}
vis_dat(df_jobs)
```

`competitors` has the most missing values. `revenue` and `founded` also look to have a significant percentage of missing values. Of these 3 variables, the most interesting and relevant is `revenue` and so we will explore the relationship of revenue to other variables to see if there is an underlying cause to the missingness of if it appears to be random.

Before focusing on a specific variable, the percentage of complete observations should be determined.

```{r}
pct_complete_case(df_jobs)
```

Less than 20% of the observations are complete, but the `competitors` variable plays very big role in this.

```{r}
pct_complete(df_jobs$competitors)
```

Almost 80% of the observations have a missing value for `competitors`, so it is going to have a big influence on the percentage of complete observations.

```{r}
df_jobs |> 
  select(-competitors) |> 
  pct_complete_case()
```

If we exclude the variable, the number of complete observations increases to 53%.

First, determine what percentage of the data has a missing value for `revenue`.

```{r}
pct_miss(df_jobs$revenue)
```

35% is a significant number. Are there any apparent patterns to the missingness or does it seem to be random?

```{r}
df_jobs |> 
  ggplot(aes(x=size, y=revenue)) +
  geom_miss_point() +
  theme(
    axis.text.x = element_text(
      angle = 90
    )
  ) +
  labs(
    x = element_blank()
  )
```

This chart shows that most of the missingness occurs for smaller organisations and that when the `size` feature is missing from an observation, the `revenue` feature is usually missing as well.

```{r}
df_jobs |> 
  filter(is.na(size)) |> 
  group_by(revenue) |> 
  count()
  
```

```{r}
df_jobs |> 
  ggplot(aes(x=type_of_ownership, y=revenue)) +
  geom_miss_point() +
  theme(
    axis.text.x = element_text(
      angle = 90
    )
  ) +
  coord_flip() +
  labs(
    x = element_blank(),
    y = element_blank()
  )
```

These two plots give some indication of patterns in the missingness. Smaller organisations appear to have a higher probability of having missing data and the type of ownership also appears important.

Exploring `size` vs `sector` also indicates that

```{r}
df_jobs |> 
  ggplot(aes(x=size, y=sector)) +
  geom_miss_point() +
  theme(
    axis.text.x = element_text(
      angle = 90
    )
  ) +
  labs(
    x = element_blank(),
    y = element_blank()
  )
```

There are

```{r}
df_jobs |> 
  filter(!is.na(size) & is.na(sector)) |> 
  group_by(size) |> 
  count()
```

```{r}
df_jobs <- df_jobs |> 
  select(-c(competitors, founded, headquarters))
```

## Creating additional features

Having a salary range for a given job is useful, but plotting ranges in large numbers will be cumbersome. To get around this problem, a salary mid-point feature, `salary_mid_point` will be generated for each job posting. In addition, to make analysis easier, the location feature will be split into two new features, `city` and `state`. This will allow for `group by` statements to be written that will allow for regional analyses to be made.

```{r}
df_jobs <- df_jobs |>
  # extract the salary range into two new variables
  extract(salary_estimate, 
          c("salary_lower_end","salary_upper_end"), 
          "\\$(\\d{2,3})K-\\$(\\d{2,3})"
  ) |> 
  # convert the new fields to numeric values
  mutate(
    salary_lower_end = as.numeric(salary_lower_end),
    salary_upper_end = as.numeric(salary_upper_end),
    salary_mid_point = round((salary_lower_end + salary_upper_end)/2,1)
  ) |> 
  extract(
    location,
    c("city","state"), 
    "^(.+),\\s([A-Z]{2})$"
  )
```

## Is the following analysis really needed?

To understand how complete each job posting is, we can use the `naniar` packages `miss_case_summary` function to count the number of missing features in each posting and from this count, the completeness of each posting can be computed.

```{r}
df_miss_count <- df_jobs |> miss_case_summary() |> select(case,pct_miss)

df_jobs <- df_jobs |> 
  mutate(
    case = row_number()
  ) |> 
  inner_join(df_miss_count, by="case") |> 
  select(-case)

rm(df_miss_count)
```

### Creating job attributes

A review of the job titles of the postings shows that not all of the positions are for data analysts. The job postings will be filtered to keep postings that meet the following rules:

* if `Data Analyst` (ignore case) is found in `job_title`
* if `Data/(another term) Analyst` (ignore case) is found in `job_title`

```{r}
df_jobs |> 
  distinct(job_title)
```

```{r}
job_filter <- c("Data Analyst", "Data\\s?\\/\\s?\\w+ Analyst")

df_jobs <- df_jobs |> 
  filter(str_detect(job_title, paste(job_filter, collapse = '|')))
```

This reduces the number of job postings to 1677, which is 74.5% of the original postings.

Now that we have eliminated job postings that are likely not data analyst positions, we can focus on determining data analyst tools are included in each posting. To do this, a set of logical features will be added, one for each tool. The tools listed are ones that are commonly used by data analysts.

```{r}
df_jobs <- df_jobs |>
  mutate(
    has_sql = if_else(str_detect(job_description, "\\bSQL\\b"), TRUE, FALSE),
    has_R = if_else(str_detect(job_description, "\\bR\\b"), TRUE, FALSE),
    has_python = if_else(str_detect(job_description, "\\b[P|p]ython\\b"), TRUE, FALSE),
    has_tableau = if_else(str_detect(job_description, "\\b[T|t]ableau\\b"), TRUE, FALSE),
    has_excel = if_else(str_detect(job_description, "\\b[E|e]xcel\\b"), TRUE, FALSE),
    has_spark = if_else(str_detect(job_description, "\\b[S|s]park\\b"), TRUE, FALSE),
    has_powerbi = if_else(str_detect(job_description, "\\b[P|p]ower\\s?BI\\b"), TRUE, FALSE),
    has_sheets = if_else(str_detect(job_description, "\\b[G|g]oogle [S|s]heets\\b"), TRUE, FALSE)
  )
```

## Univariate analysis

Now that the data have been cleaned and new features created, an exploration of univariate data will be done on key features in the dataset. The goal is to understand how the data are distributed, if there are consistency issues, typos, or outliers that need to be investigated.

### `salary_mid_point`

```{r}
df_jobs |> 
  ggplot(aes(x=salary_mid_point)) +
  geom_histogram(binwidth = 10, colour="white", fill=default_fill_colour) +
  labs(
    x="Salary mid point",
    y="Count",
    title="Distribution of job posting salary"
  ) +
  theme(
    plot.title.position = "plot"
  )

```
While the data isn't normally distributed, it is pretty close. There is a positive skew to the data. That is not surprising.

There is an observation that is missing salary. Since it is a single observations, it will be dropped from the dataset.

```{r}
df_jobs <- df_jobs |> 
  filter(!is.na(salary_mid_point))
```

### `state`

The plot of job postings by state does not reveal any issues with the factor level. The state codes all look correct. The distribution of jobs does raise some questions. While it is not surprising that both California and Texas have a lot of job postings, the very small number in states like Georgia is surprising. More research would need to be conducted to determine why this might be the case. The source of the data does not indicate the methods for collecting the data.

```{r}
df_jobs |> 
  ggplot(aes(x=fct_rev(fct_infreq(state)))) +
    geom_bar(fill=default_fill_colour) +
    coord_flip() +
    labs(
      x=element_blank(),
      y="Job count",
      title="Data analyst job posting count by state"
    )
```

### `size`

While about 10% of the job postings do not disclose the size of the organisation, the number of job postings for each organisation size range is reasonably consistent. Both large and small organisations are looking to fill data analyst positions with no one size of organisation being dominant. This suggests that organisations of all sizes believe that data analysts are important to their success.

```{r}
df_jobs |> 
  ggplot(aes(x=fct_rev(size))) +
  geom_bar(fill=default_fill_colour) +
  coord_flip() +
  labs(
    x=element_blank(),
    y="Job posting count",
    title="Data analyst job postings by organisation size"
  ) +
  theme(
    plot.title.position = "plot"
  )
```

### `type_of_ownership`

The `type_of_ownership` suggests that the data from Glassdoor.com is not a representative sample of the population of data analyst jobs. The data are very heavily drawn from private companies. The table below shows the percentage of jobs by type of ownership, and private companies account for over half of the job postings. Public companies (21%) and `NA` (7.9%) and for the majority of the remaining listings.

```{r}
df_jobs |> 
  ggplot(aes(x=fct_rev(fct_infreq(type_of_ownership)))) +
  geom_bar(fill=default_fill_colour) +
  coord_flip() +
  labs(
    x=element_blank(),
    y="Job posting count",
    title="Data analyst job postings by type of ownership"
  ) +
  theme(
    plot.title.position = "plot"
  )
```

```{r}
df_jobs |> 
  group_by(type_of_ownership) |> 
  summarise(
    total = n()
  ) |> 
  mutate(
    pct_total = round(total/sum(total)*100, 1)
  ) |> 
  arrange(desc(pct_total))
```

### `sector`

```{r}
df_jobs |> 
  filter(!is.na(sector)) |> 
  ggplot(aes(x=fct_rev(fct_infreq(sector)))) +
  geom_bar(fill=default_fill_colour) +
  coord_flip() +
  labs(
    x=element_blank(),
    y="Job posting count",
    title="IT and Business Services are leading sectors for data analyst positions",
    caption="Source: Glassdoor.com"
  ) +
  theme(
    plot.title.position = "plot"
  )
```
There are a few dominant sectors in this dataset and large number of observations that are missing the sector feature. The two leading sectors are Information Technology and Business Services. Finance and Health Care are the next largest categories.

### `rating`

There are 216 job postings that are missing a `rating` value. The feature is close to normally distributed but there are spikes at the extremes. There is a very pronounced spike at 5 and to a much smaller degree at 1. This is likely a result of human psychology. If a company is highly regarded, it seems plausible that instead of rating it a 4, people rate it a 5. The effect seems much smaller when giving a company a negative rating.

```{r}
df_jobs |> 
  filter(!is.na(rating)) |> 
  ggplot(aes(x=rating)) +
  geom_histogram(binwidth = 0.3,colour="white",fill=default_fill_colour) +
  labs(
    x="Company rating",
    y="Count",
    title="Company ratings follow a roughly normal distrbution",
    caption="Source: Glassdoor.com"
  ) +
  theme(
    plot.title.position = "plot"
  )
```
### `salary_mid_point`

```{r}
df_jobs |> 
  ggplot(aes(x=salary_mid_point)) +
  geom_histogram(binwidth = 10,colour="white",fill=default_fill_colour) +
  labs(
    x="Salary midpoint",
    y="Count",
    title="Distribution of salary midpoint for all job postings",
    subtitle="Salaries in thousands of USD",
    caption="Source: Glassdoor.com"
  ) +
  theme(
    plot.title.position = "plot"
  )

```

## Do we need to include this analysis??

**pct_miss**

```{r}
df_jobs |> 
  ggplot(aes(x=pct_miss)) +
  geom_histogram()
```

```{r}
df_jobs |> 
  group_by(type_of_ownership) |> 
  summarise(
    count = n(),
    med_pct_miss = round(median(pct_miss),1)
  ) |> 
  arrange(med_pct_miss) |> 
  gt()
```

## Bivariate analysis

### `salary_mid_point` vs `rating`

There is only a very weak positive correlation between `rating` and `salary_mid_point`, indicating that compensation isn't a significant factor in an organisation's rating. It also means that choosing an organisation because of how well rated it is will not translate into a higher salary, if there is a linear relationship between `rating` and `salary_mid_point`. There is no obvious trend, linear or nonlinear in the plot below.

```{r}
df_jobs |> 
  filter(!is.na(rating)) |> 
  ggplot(aes(x=rating,y=salary_mid_point)) +
  geom_jitter(alpha=0.7) +
  geom_smooth(method = "lm")
```
The correlation coefficient is quite small, but positive.

```{r}
cor(df_jobs$rating, df_jobs$salary_mid_point, use = "complete.obs")
```
### `salary_mid_point` vs `size`

Comparing boxplots of `salary_mid_point` vs `size` shows that there is not a significant difference in salaries based on organisation size.

```{r}
df_jobs |> 
  filter(!is.na(size)) |> 
  ggplot(aes(x=size, y=salary_mid_point)) +
  geom_boxplot(linewidth=1) +
  geom_jitter(alpha=0.4) +
  scale_colour_brewer(palette = "Dark2") +
  labs(
    x="Organisation number of employees",
    y="Salary midpoint (USD)",
    caption = "Source: Glassdoor.com",
    title="Organisation size has little impact on salary"
  ) +
  theme(
    legend.position = "none"
  )
```

```{r}
df_jobs |> 
  filter(!is.na(size)) |> 
  ggplot(aes(x=salary_mid_point)) +
  geom_histogram(binwidth = 10, colour="white", fill=default_fill_colour) +
  facet_wrap(~size)
```
We can use a general linear model to compare the `size` groups.

```{r}
df_jobs |> 
  filter(!is.na(size)) |>
  lm(salary_mid_point ~ size, data=_) |> 
  summary()
```
This result confirms what we could see in the boxplots. There is no significant difference between the groups. The *p-value* for the test is $p=0.631$ and the estimates for all groups are small and have high *p-values*. 

### `type_of_ownership` vs `salary_mid_point`

As with `size`, there is not a significant difference in salary between groups.

```{r}
df_jobs |> 
  filter(!is.na(type_of_ownership)) |> 
  group_by(type_of_ownership) |> 
  count() |> 
  filter(n > 15) |>  
  inner_join(df_jobs, by="type_of_ownership") |> 
  ggplot(aes(x=type_of_ownership, y=salary_mid_point)) +
  geom_jitter(alpha=0.2) +
  geom_boxplot(linewidth=1,fill="transparent",outlier.shape = NA) +
  scale_colour_brewer(palette = "Dark2") +
  coord_flip() +
  labs(
    x=element_blank(),
    y="Salary midpoint",
    caption = "Source: Glassdoor.com",
    title="Ownership type has little impact on salary",
    subtitle="salaries in USD"
  ) +
  theme(
    legend.position = "none"
  )
```
```{r}
df_jobs |> 
  filter(!is.na(type_of_ownership)) |> 
  group_by(type_of_ownership) |> 
  count() |> 
  filter(n > 15) |>  
  inner_join(df_jobs, by="type_of_ownership") |> 
  lm(salary_mid_point~type_of_ownership,data=_) |> 
  summary()
```
The is one interesting result here, that the Government group is significantly different to the others. The box plots hint at this, and the linear model confirms it at the 95% confidence level.

## Popular tools and technologies

An analysis of technology keyword occurrences shows that SQL is the most frequently requested technology skill. SQL appears in 64.8% of all job postings. Other popular skills are Excel, Python, Tableau, and R.

```{r}
df_da_tools <- df_jobs |> 
  summarise(
    SQL=compute_percentage(has_sql, n()),
    R=compute_percentage(has_R, n()),
    Python=compute_percentage(has_python, n()),
    Excel=compute_percentage(has_excel, n()),
    Tableau=compute_percentage(has_tableau, n()),
    Spark=compute_percentage(has_spark, n()),
    PowerBI=compute_percentage(has_powerbi, n()),
    Sheets=compute_percentage(has_sheets, n())
  ) |> 
  pivot_longer(cols = everything(), names_to = "technology", values_to = "pct_of_jobs")

df_da_tools |> 
  mutate(
    technology = fct_reorder(technology, pct_of_jobs)
  ) |> 
  ggplot(aes(x=technology, y=pct_of_jobs)) +
  geom_col(fill=default_fill_colour) +
  coord_flip() +
  labs(
    x=element_blank(),
    y="Percent of all job postings",
    title="SQL & Excel are the most requested skills",
    subtitle="Popularity of standard data analyst tools"
  ) +
  theme(
    plot.title.position = "plot"
  )
```

### Do jobs requiring SQL pay more?

Do job postings requiring SQL have a higher salary than those that don't? The data do not support the idea. Job posting that list SQL as a desired skill have a very similar median to those that do not. The salary range is tighter for jobs that include SQL, but there is clearly no salary advantage. There are, however, far more jobs that list SQL than do not, so it is clearly an important skill.

```{r}
df_jobs |> 
  ggplot(aes(x=has_sql,y=salary_mid_point)) +
  geom_jitter(alpha=0.1) +
  geom_boxplot(linewidth=0.75, fill="transparent", outlier.shape = NA) +
  labs(
    x="Includes SQL",
    y="Salary midpoint",
    title="Comparison of job posting salaries",
    subtitle="Salaries (USD) for jobs requiring SQL"
  ) +
  theme(
    plot.title.position = "plot"
  )
  
```

### Does R or Python command more pay?

The two primary coding languages used by data analysts are Python and R. These skills are requested in 497 and 351 job listings, respectively. Both languages are included in 288 job listings. Python is the more popular of the two in this dataset. One potential issue here is the bias in this dataset to private companies. R is particularly popular in the medical space and in academia. If those sectors are underrepresented in this dataset, then the results of this comparison may be misleading. 

```{r}
df_jobs |> 
  summarise(
    R = sum(has_R),
    Python = sum(has_python),
    Both = sum(has_R&has_python)
  ) |> 
  pivot_longer(cols = everything(), names_to = "Language", values_to = "Jobs")
```

Filtering out the job postings that list both languages, the salaries of R-only and Python-only jobs can be compared.

```{r}
df_jobs |> 
  filter(xor(has_python,has_R)) |> 
  mutate(
    coding_lang = if_else(has_python, "Python", "R")
  ) |> 
  ggplot(aes(x=coding_lang,y=salary_mid_point)) +
  geom_jitter(alpha=0.1) +
  geom_boxplot(linewidth=0.75, fill="transparent", outlier.shape = NA) +
  labs(
    x="Coding language",
    y="Salary midpoint",
    title="Comparison of job posting salaries",
    subtitle="Salaries (USD) for jobs requiring R or Python"
  ) +
  theme(
    plot.title.position = "plot"
  )
```
The Python-only jobs have a noticeably higher mean salary and a wider range of salaries in general. The mean for Python-only jobs is about \$75.8K while for R-only jobs the mean is about \$65.7K

```{r}
df_jobs |> 
  filter(xor(has_python,has_R)) |> 
  mutate(
    coding_lang = if_else(has_python, "Python", "R")
  ) |> 
  lm(salary_mid_point ~ coding_lang, data=_) |> 
  summary()
```


## Findings


## Summary and recommendations




