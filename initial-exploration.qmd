---
title: "Initial Exploration & Cleaning"
author: "Tripp Bishop"
format: html
editor: visual
---

```{r setup}
#| echo: false
#| message: false
library(tidyverse)
library(janitor)
library(visdat)
library(naniar)
library(gt)
theme_set(theme_minimal())
```

## Data Analyst Jobs Dataset

In the initial phase of the analysis, it is important to import the data, take a quick look at the first few records, and then glimpse the dataset to get a better sense of its structure.

```{r}
#| message: false

df_jobs <- read_csv("data/DataAnalyst.csv")
head(df_jobs)
```

Right away we can see that we have a superfluous variable, ...1, and that at least two columns contain "-1". There is a good chance that "-1" is the NA placeholder for this dataset. This will need to be addressed after the initial exploration of the data is complete. There are a several variables that will need to be converted to factors, some nominal and some ordinal. There are also some variables with non-standard names. These should be renamed so that they are easier to work with in the code.

```{r}
glimpse(df_jobs)
```

Start by renaming the variables and removing the row id variable. It won't be necessary.

```{r rename variables}
df_jobs <- df_jobs |> clean_names() |> select(-x1)
```

## Exploring potential factors variables

The `size`, `type_of_ownership`, `industry`, `sector`, and `revenue` variables are good candidates for conversion to factors. The first step is understanding how many distinct values each variable has.

**size**

```{r}
df_jobs |> distinct(size)
```

There is a -1 which indicates missingness. Furthermore, the value of "Unknown" is not helpful, so it should be converted to `NA` as well to facilitate analysis.

**industry**

```{r}
df_jobs |> distinct(industry)
```

There is a -1 which indicates missingness.

**type_of_ownership**

```{r}
df_jobs |> distinct(type_of_ownership)
```

In addition to -1, `type_of_ownership` has a value of "Unknown". These will need to be cleaned up and converted to `NA` values.

**sector**

```{r}
df_jobs |> distinct(sector)
```

In addition to -1, `sector` has a value of "Unknown". These will need to be cleaned up and converted to `NA` values.

**revenue**

```{r}
df_jobs |> distinct(revenue)
```

There is no -1 value in `revenue`, but the value "Unknown / Non-Applicable" is present. This will be converted to `NA` for reasons state above.

## Exploring remaining variables

The remaining variables will need to be examined for missingness, although they will not be converted to factors.

**founded**

```{r}
df_jobs |> distinct(founded) |> arrange(founded)
```

`founded` is a numeric variable, but does have -1 as a value.

**rating**

```{r}
df_jobs |> distinct(rating) |> arrange(rating)
```

`rating` is a numeric variable that appears to have a valid range of 1 to 5. The value -1 is present and will be converted to `NA`.

**location**

```{r}
df_jobs |> distinct(location) |> arrange(location)
```

`location` does not contain any "-1" values, but does contain what appear to be a few empty values that will need to be cleaned up.

**headquarters**

```{r}
df_jobs |> distinct(headquarters) |> arrange(headquarters)
```

`headquarters` contains "-1" as a value as well as some white-space values.

**salary_estimate**

```{r}
df_jobs |> distinct(salary_estimate) |> arrange(salary_estimate)
```

`salary_estimate` does contain "-1" as a value. Every value also ends with the string "(Glassdoor est.)". In addition to this, the field contains the range from low to high in the formation \$AK - \$BK, where A & B are 2-3 digit integers. This field should be cleaned and then split into `salary_lower_end` and `salary_upper_end` numeric variables.

**company_name**

```{r}
df_jobs |> distinct(company_name) |> arrange(company_name)
```

The `company_name` field has a newline character followed by what appears to be the rating value. This should be removed. There is the value "1", this is probably a missing value.

## Addressing missing values

The `easy_apply` variable should be a logical field. Rather than having `NA` for FALSE values, simply set all "-1" values to FALSE and all others to TRUE. This will force the `easy_apply` to be a logical variable. Next, the -1 and "Unknown" values can also be mapped to `NA` to complete the task. We can use features of the `naniar` package to understand the missing values much better if they are all set to `NA` rather than a combination of `NA` and "Unknown".

```{r}
df_jobs <- df_jobs |> 
  mutate(
    easy_apply = if_else(easy_apply == "-1", FALSE, TRUE),
  ) |> 
  replace_with_na(df_jobs, 
                  replace=list(
                    salary_estimate = "-1",
                    rating = -1,
                    headquarters = -1,
                    founded = -1,
                    size = c("-1", "Unknown"),
                    type_of_ownership = c("Unknown","-1"),
                    industry = "-1",
                    sector = "-1",
                    revenue = c("-1", "Unknown / Non-Applicable"),
                    competitors = "-1"
                  )
  )
```

## Cleaning variables

Now that missing values have been standardised, the other data issues that have been identified can be addressed. `company_name` will have the superfluous characters removed, several factors will be created, and the `salary_estimate` data will be extracted into two new variables: `salary_lower_end` and `salary_upper_end`.

The factors are quite wordy and this will impact the visualisation of data using these variables, so prior to creating factors, it will be good to simplify the values without obscuring their meaning.

```{r}
df_jobs <- df_jobs |> 
  mutate(
    size = str_remove(size, "\\semployees"),
    revenue = str_remove(revenue, "\\s\\(USD\\)"),
    revenue = str_replace(revenue, "\\sbillion", "B"),
    revenue = str_replace(revenue, "\\smillion", "M")
  )
```

```{r}
df_jobs <- df_jobs |> 
  mutate(
    # drop the \n + rating information from the company name
    company_name = str_replace(company_name, "\\n.+$", ""),
    # make industry, sector, any type_of_ownership nominal factors
    industry = as_factor(industry),
    sector = as_factor(sector),
    type_of_ownership = as_factor(type_of_ownership),
    # make size and revenue ordinal factors
    size = fct_relevel(size, 
                       "1 to 50",
                       "51 to 200",
                       "201 to 500",
                       "501 to 1000",
                       "1001 to 5000",
                       "5001 to 10000",
                       "10000+"),
    revenue = fct_relevel(revenue, 
                       "Less than $1M",
                       "$1 to $5M",
                       "$5 to $10M",
                       "$10 to $25M",
                       "$25 to $50M",
                       "$50 to $100M",
                       "$100 to $500M",
                       "$500M to $1B",
                       "$1 to $2B",
                       "$2 to $5B",
                       "$5 to $10B",
                       "$10+B")
  ) |> 
  # extract the salary range into two new variables
  extract(salary_estimate, 
          c("salary_lower_end","salary_upper_end"), 
          "\\$(\\d{2,3})K-\\$(\\d{2,3})"
  ) |> 
  mutate(
    salary_lower_end = as.numeric(salary_lower_end),
    salary_upper_end = as.numeric(salary_upper_end)
  )
```

## Explore missing values

To begin, the `vis_dat` function from the `visdat` package is used to get a quick look at where missing values are and roughly how frequently they occur in each variable.

```{r}
vis_dat(df_jobs)
```

`competitors` has the most missing values. `revenue` and `founded` also look to have a significant percentage of missing values. Of these 3 variables, the most interesting and relevant is `revenue` and so we will explore the relationship of revenue to other variables to see if there is an underlying cause to the missingness of if it appears to be random.

Before focusing on a specific variable, the percentage of complete observations should be determined.

```{r}
pct_complete_case(df_jobs)
```

Less than 20% of the observations are complete, but the `competitors` variable plays very big role in this.

```{r}
pct_complete(df_jobs$competitors)
```

Almost 80% of the observations have a missing value for `competitors`, so it is going to have a big influence on the percentage of complete observations.

```{r}
df_jobs |> 
  select(-competitors) |> 
  pct_complete_case()
```

If we exclude the variable, the number of complete observations increases to 53%.

First, determine what percentage of the data has a missing value for `revenue`.

```{r}
pct_miss(df_jobs$revenue)
```

35% is a significant number. Are there any apparent patterns to the missingness or does it seem to be random?

```{r}
df_jobs |> 
  ggplot(aes(x=size, y=revenue)) +
  geom_miss_point() +
  theme(
    axis.text.x = element_text(
      angle = 90
    )
  ) +
  labs(
    x = element_blank()
  )
```

This chart shows that most of the missingness occurs for smaller organisations and that when the `size` feature is missing from an observation, the `revenue` feature is usually missing as well.

```{r}
df_jobs |> 
  filter(is.na(size)) |> 
  group_by(revenue) |> 
  count()
  
```

```{r}
df_jobs |> 
  ggplot(aes(x=type_of_ownership, y=revenue)) +
  geom_miss_point() +
  theme(
    axis.text.x = element_text(
      angle = 90
    )
  ) +
  coord_flip() +
  labs(
    x = element_blank(),
    y = element_blank()
  )
```

These two plots give some indication of patterns in the missingness. Smaller organisations appear to have a higher probability of having missing data and the type of ownership also appears important.

Exploring size vs sector also indicates that

```{r}
df_jobs |> 
  ggplot(aes(x=size, y=sector)) +
  geom_miss_point() +
  theme(
    axis.text.x = element_text(
      angle = 90
    )
  ) +
  labs(
    x = element_blank(),
    y = element_blank()
  )
```

There are

```{r}
df_jobs |> 
  filter(!is.na(size) & is.na(sector)) |> 
  group_by(size) |> 
  count()
```

```{r}
df_jobs <- df_jobs |> 
  select(-c(competitors, founded, headquarters))
```

## Creating additional features

Having a salary range for a given job is useful, but plotting ranges in large numbers will be cumbersome. To get around this problem, a salary mid-point feature can be generated for each job. In addition, to make analysis easier, the location feature will be split into two new features, `city` and `state`. This will allow for `group by` statements to be written that will allow for regional analyses to be made.

```{r}
df_jobs <- df_jobs |> 
  mutate(
    salary_mid_point = round((salary_lower_end + salary_upper_end)/2,1)
  ) |> 
  extract(location,
          c("city","state"), 
          "^(.+), ([A-Z]{2})$")
```

To understand how complete each job posting is, we can use the `naniar` packages `miss_case_summary` function to count the number of missing features in each posting and from this count, the completeness of each posting can be computed.

```{r}
df_miss_count <- df_jobs |> miss_case_summary() |> select(case,pct_miss)

df_jobs <- df_jobs |> 
  mutate(
    case = row_number()
  ) |> 
  inner_join(df_miss_count, by="case") |> 
  select(-case)

rm(df_miss_count)
```

### Creating job attributes

A review of the job titles of the postings shows that not all of the positions are for data analysts. The job postings will be filtered to keep postings that meet the following rules:

* if `Data Analyst` (either case) is found in `job_title`
* if `Data/(another term) Analyst` (either case) is found in `job_title`

```{r}
df_jobs |> 
  distinct(job_title) |> 
  View()
```

```{r}
job_filter <- c("Data Analyst", "Data\\s?\\/\\s?\\w+ Analyst")

df_jobs <- df_jobs |> 
  filter(str_detect(job_title, paste(job_filter, collapse = '|')))
```
This reduces the number of job postings to 1677, which is 74.5% of the original postings.

Now that we have eliminated job postings that are likely not data analyst positions, we can focus on determining data analyst tools are included in each posting. To do this, a set of logical features will be added, one for each tool.

```{r}
df_jobs <- df_jobs |>
  mutate(
    has_sql = if_else(str_detect(job_description, "\\bSQL\\b"), TRUE, FALSE),
    has_R = if_else(str_detect(job_description, "\\bR\\b"), TRUE, FALSE),
    has_python = if_else(str_detect(job_description, "\\b[P|p]ython\\b"), TRUE, FALSE),
    has_tableau = if_else(str_detect(job_description, "\\b[T|t]ableau\\b"), TRUE, FALSE),
    has_excel = if_else(str_detect(job_description, "\\b[E|e]xcel\\b"), TRUE, FALSE),
    has_spark = if_else(str_detect(job_description, "\\b[S|s]park\\b"), TRUE, FALSE),
    has_powerbi = if_else(str_detect(job_description, "\\b[P|p]ower\\s?BI\\b"), TRUE, FALSE),
    has_sheets = if_else(str_detect(job_description, "\\b[G|g]oogle [S|s]heets\\b"), TRUE, FALSE)
  )
```

## Univariate analysis

**salary_mid_point**

```{r}
df_jobs |> 
  ggplot(aes(x=salary_mid_point)) +
  geom_histogram(binwidth = 10, colour="white") +
  labs(
    x="Salary mid point",
    y="Count",
    title="Distribution of job posting salary"
  ) +
  theme(
    plot.title.position = "plot"
  )

```
While the data isn't normally distributed, it is pretty close. There is a positive skew to the data. That is not surprising.

There is an observation that is missing salary. Since it is a single observations, it will be dropped from the dataset.

```{r}
df_jobs <- df_jobs |> 
  filter(!is.na(salary_mid_point))
```

**state**

The plot of job postings by state does not reveal any issues with the factor level. The state codes all look correct. The distribution of jobs does raise some questions. While it is not surprising that both California and Texas have a lot of job postings, the very small number in states like Georgia is surprising. More research would need to be conducted to determine why this might be the case. The source of the data does not indicate the methods for collecting the data.

```{r}
df_jobs |> 
  ggplot(aes(x=fct_rev(fct_infreq(state)))) +
    geom_bar() +
    coord_flip() +
    labs(
      x=element_blank(),
      y="Job count",
      title="Data analyst job posting count by state"
    )
```

**size**

While about 10% of the job postings do not disclose the size of the organisation, the number of job postings for each organisation size range is reasonably consistent. Both large and small organisations are looking to fill data analyst positions with no one size of organisation being dominant. This suggests that organisations of all sizes believe that data analysts are important to their success.

```{r}
df_jobs |> 
  ggplot(aes(x=fct_rev(size))) +
  geom_bar() +
  coord_flip() +
  labs(
    x=element_blank(),
    y="Job posting count",
    title="Data analyst job postings by organisation size"
  ) +
  theme(
    plot.title.position = "plot"
  )
```

**type_of_ownership**

The `type_of_ownership` suggests that the data from Glassdoor.com is not a representative sample of the population of data analyst jobs. The data are very heavily drawn from private companies. The table below shows the percentage of jobs by type of ownership, and private companies account for over half of the job postings. Public companies (21%) and `NA` (7.9%) and for the majority of the remaining listings.

```{r}
df_jobs |> 
  ggplot(aes(x=fct_rev(fct_infreq(type_of_ownership)))) +
  geom_bar() +
  coord_flip() +
  labs(
    x=element_blank(),
    y="Job posting count",
    title="Data analyst job postings by type of ownership"
  ) +
  theme(
    plot.title.position = "plot"
  )
```

```{r}
df_jobs |> 
  group_by(type_of_ownership) |> 
  summarise(
    total = n()
  ) |> 
  mutate(
    pct_total = round(total/sum(total)*100, 1)
  ) |> 
  arrange(desc(pct_total))
```

**sector**

```{r}
df_jobs |> 
  ggplot(aes(x=sector)) +
  geom_bar() +
  coord_flip() +
  labs(
    x=element_blank(),
    y="Job posting count",
    title="Data analyst job postings by sctor",
    caption="Glassdoor.com"
  ) +
  theme(
    plot.title.position = "plot"
  )
```
There are a few dominant sectors in this dataset and large number of observations that are missing the sector feature. The two leading sectors are Information Technology and Business Services. `NA`, Finance, and Health Care are the next largest categories.

**rating**

There are 216 job postings that are missing a `rating` value. The feature is close to normally distributed but there are spikes at the extremes. There is a very pronounced spike at 5 and to a much smaller degree at 1. This is likely a result of human psychology. If a company is highly regarded, it seems plausible that instead of rating it a 4, people rate it a 5. The effect seems much smaller when giving a company a negative rating.
```{r}
df_jobs |> 
  filter(!is.na(rating)) |> 
  ggplot(aes(x=rating)) +
  geom_histogram(binwidth = 0.3,colour="white") +
  labs(
    x="Company rating",
    y="count",
    title="Distribution of companies by rating",
    caption="Source: Glassdoor.com"
  ) +
  theme(
    plot.title.position = "plot"
  )
```
**salary_mid_point**

```{r}
df_jobs |> 
  ggplot(aes(x=salary_mid_point)) +
  geom_histogram(binwidth = 10,colour="white") +
  labs(
    x="Salary midpoint",
    y="Count",
    title="Distribution of salary midpoint for all job postings",
    subtitle="in thousands of USD",
    caption="Source: Glassdoor.com"
  ) +
  theme(
    plot.title.position = "plot"
  )

```

**pct_miss**

```{r}
df_jobs |> 
  ggplot(aes(x=pct_miss)) +
  geom_histogram()
```

```{r}
df_jobs |> 
  group_by(type_of_ownership) |> 
  summarise(
    count = n(),
    med_pct_miss = round(median(pct_miss),1)
  ) |> 
  arrange(med_pct_miss) |> 
  gt()
```

## Bivariate analysis

### `salary_mid_point` vs `rating`

There is only a very weak positive correlation between `rating` and `salary_mid_point`, indicating that compensation isn't a significant factor in an organisation's rating. It also means that choosing an organisation because of how well rated it is will not translate into a higher salary, if there is a linear relationship between `rating` and `salary_mid_point`. There is no obvious trend, linear or nonlinear in the plot below.

```{r}
df_jobs |> 
  filter(!is.na(rating)) |> 
  ggplot(aes(x=rating,y=salary_mid_point)) +
  geom_jitter() +
  geom_smooth(method = "lm")
```
The correlation coefficient is quite small, but positive.

```{r}
cor(df_jobs$rating, df_jobs$salary_mid_point, use = "complete.obs")
```
### `salary_mid_point` vs `size`




```{r}
df_jobs |> 
  filter(!is.na(size)) |> 
  ggplot(aes(x=size, y=salary_mid_point, colour=size)) +
  geom_boxplot(linewidth=1) +
  geom_jitter(alpha=0.4) +
  scale_colour_brewer(palette = "Dark2") +
  labs(
    x="Organisation number of employees",
    y="Salary midpoint",
    caption = "Source: Glassdoor.com",
    title="Distribution of salary by organisation size",
    subtitle="salaries in USD"
  ) +
  theme(
    legend.position = "none"
  )
```

```{r}

df_jobs |> 
  filter(!is.na(type_of_ownership)) |> 
  group_by(type_of_ownership) |> 
  count() |> 
  filter(n > 15) |>  
  inner_join(df_jobs, by="type_of_ownership") |> 
  ggplot(aes(x=type_of_ownership, y=salary_mid_point, colour=type_of_ownership)) +
  geom_boxplot(linewidth=1) +
  geom_jitter(alpha=0.4) +
  scale_colour_brewer(palette = "Dark2") +
  coord_flip() +
  labs(
    x=element_blank(),
    y="Salary midpoint",
    caption = "Source: Glassdoor.com",
    title="Distribution of salary by ownership type",
    subtitle="salaries in USD"
  ) +
  theme(
    legend.position = "none"
  )
```


## Populare tools and technologies


## Findings


## Summary and recommendations




