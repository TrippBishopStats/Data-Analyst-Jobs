---
title: "Exploration, Cleaning, and Analysis"
author: "Tripp Bishop"
format: html
date: 15 JAN 2023
---

## Data Analyst Jobs Dataset

The Data Analyst Jobs dataset was produced by scraping *Glassdoor.com* data analyst job postings. The date that this was done is not known, but the covid-19 pandemic is referenced so a likely lower bound on the date is mid-2020.


```{r setup}
#| message: false
library(tidyverse)
library(janitor)
library(visdat)
library(naniar)
library(gt)

my_theme <- theme_minimal() + theme(
  plot.title.position = "plot"
)
theme_set(my_theme)
source("scripts/helper-functions.R")
default_fill_colour <- "#1f63b4"
```

In the initial phase of the analysis, it is important to import the data, take a quick look at the first few records, and then glimpse the dataset to get a better sense of its structure.

```{r read in data}
#| message: false

df_jobs <- read_csv("data/DataAnalyst.csv")
head(df_jobs)
```

Right away we can see that we have a superfluous variable, `...1`, and that at least two columns contain "-1". It is likely that "-1" is the NA placeholder for this dataset. This will need to be addressed after the initial exploration of the data is complete. There are a several variables that will need to be converted to factors, some nominal and some ordinal. There are also some variables with non-standard names. These should be renamed so that they are easier to work with in the code.

```{r glimpse dataset}
glimpse(df_jobs)
```

Begin by renaming the variables and removing the row id variable.

```{r rename variables}
df_jobs <- df_jobs |> clean_names() |> select(-x1)
```

## Exploring potential factors variables

The `size`, `type_of_ownership`, `industry`, `sector`, and `revenue` variables are good candidates for conversion to factors. The first step is understanding how many distinct values each variable has.

### `size`

This field contains categorical data about the size of the company listing the job.

```{r distinct sizes}
df_jobs |> distinct(size)
```

There is a -1 which indicates missingness. In addition to "-1", there is the value "Unknown" which will be retained and it will be the standard across all categorical variables. We want to have such a level in our factors so that if we want to perform any statistical test we will not have to contend with `NA` values which will cause the tests problems.

### `industry`

This field contains categorical data about the industry of the company listing the job.

```{r distinct industries}
df_jobs |> distinct(industry)
```

There is a "-1" which indicates missingness. It will be converted to "Unknown" during the cleaning phase.

### `type_of_ownership`

This feature indicates the ownership model of the company listing the job.

```{r distinct ownership types}
df_jobs |> distinct(type_of_ownership)
```

The feature contains "-1. It will be converted to "Unknown" during the cleaning phase.

### `sector`

`sector` is related to the `industry` feature but is more general.

```{r distinct sectors}
df_jobs |> distinct(sector)
```

The feature contains "-1. It will be converted to "Unknown" during the cleaning phase.

### `revenue`

This is a categorical feature that indicates how much revenue the company listing the job has reported. The levels of this feature are not of uniform width.

```{r distinct revenue}
df_jobs |> distinct(revenue)
```

There is no -1 value in `revenue`, but the value "Unknown / Non-Applicable" is present. This will be converted to "Unkown" so that it is the same as the other features.

## Exploring remaining features

The remaining features will need to be examined for missingness, although they will not be converted to factors.

### `founded`

This feature contains the year the company was founded. It will not be used in this analysis and will be dropped from the dataset.

### `rating`

`rating` is a numeric variable that appears to have a valid range of 1 to 5.

```{r distinct ratings}
df_jobs |> distinct(rating) |> arrange(rating)
```

 The value -1 is present and will be converted to `NA`.

### `location`

This feature indicates the city and state where the job is located. It contains two pieces of data, the city and state, so in order to make the data tidy, this field will be broken up into two new fields, `city` and `state`.

```{r distinct locations}
df_jobs |> distinct(location) |> arrange(location)
```

`location` does not contain any "-1" values.

### `headquarters`

This feature contains information about where the headquarters of the listing company is located. It will not be used in the analysis and will be dropped.

### `salary_estimate`

This feature contain the salary range of the job listing reported in thousands of dollars.

```{r distinct salary estimates}
df_jobs |> distinct(salary_estimate) |> arrange(salary_estimate)
```

`salary_estimate` does contain "-1" as a value. Every value also ends with the string "(Glassdoor est.)". In addition to this, the field contains the range from low to high in the formation \$AK - \$BK, where A & B are 2-3 digit integers. This field should be cleaned and then split into `salary_lower_end` and `salary_upper_end` numeric variables.

```{r}
df_jobs |> 
  filter(salary_estimate == "-1") |> 
  count()
```
The observation with "-1" for `salary_estimate` will be dropped from the dataset. I will not attempt to impute the value. This is reasonable give that it is only one record. If a substantial number of observations were missing values for this feature we would need to take another course of action. Imputing the values might be possible, but more likely, a new data source would need to be found. 

### `company_name`

The name of the company.

```{r distinct company names}
df_jobs |> distinct(company_name) |> arrange(company_name)
```

The `company_name` feature has many values containing a newline character followed by what appears to be the rating value. This should be removed. There is a company with the name "1". This could be a mistake, but the feature will not factor into any of the analysis

## Addressing missing values

The `easy_apply` variable should be a logical field. Rather than having `NA` for FALSE values, simply set all "-1" values to `FALSE` and all others to `TRUE`. This will force the `easy_apply` to be a logical variable. Next, the -1 and "Unknown" values can also be mapped to `NA` to complete the task. We can use features of the `naniar` package to understand the missing values much better if they are all set to `NA` rather than a combination of `NA` and "Unknown".

```{r cleaning missing values}
df_jobs <- df_jobs |> 
  mutate(
    easy_apply = if_else(easy_apply == "-1", FALSE, TRUE),
  ) |> 
  replace_with_na(df_jobs, 
                  replace=list(
                    salary_estimate = "-1",
                    rating = -1,
                    size = c("-1", "Unknown"),
                    type_of_ownership = c("Unknown","-1"),
                    industry = "-1",
                    sector = "-1",
                    revenue = c("-1", "Unknown / Non-Applicable")
                  )
  )
```

## Cleaning variables

Now that missing values have been standardised, the other data issues that have been identified can be addressed. `company_name` will have the superfluous characters removed, several factors will be created, and the `salary_estimate` data will be extracted into two new variables: `salary_lower_end` and `salary_upper_end`.

The factors are quite wordy and this will impact the visualisation of data using these variables, so prior to creating factors, it will be good to simplify the values without obscuring their meaning.

```{r updating revenue and size categories}
df_jobs <- df_jobs |> 
  mutate(
    # drop the \n + rating information from the company name
    company_name = str_replace(company_name, "\\n.+$", ""),
    size = str_remove(size, "\\semployees"),
    revenue = str_remove(revenue, "\\s\\(USD\\)"),
    revenue = str_replace(revenue, "\\sbillion", "B"),
    revenue = str_replace(revenue, "\\smillion", "M")
  )
```

## Remove unnecessary features

3 features will be dropped as they will not play a role in the analysis: `competitors`, `founded`, and `headquarters`.

```{r remove unneeded features}
# drop features that will not be used in the analysis
df_jobs <- df_jobs |> 
  select(-c(competitors, founded, headquarters))
```

## Explore missing values

To begin, the `vis_dat` function from the `visdat` package is used to get a quick look at where missing values are and roughly how frequently they occur in each variable.

```{r visualising missingness}
vis_dat(df_jobs)
```

`competitors` has the most missing values. `revenue` and `founded` also look to have a significant percentage of missing values. Of these 3 variables, the most interesting and relevant is `revenue` and so we will explore the relationship of revenue to other variables to see if there is an underlying cause to the missingness of if it appears to be random.

Before focusing on a specific variable, the percentage of complete observations should be determined.

```{r}
pct_complete_case(df_jobs)
```

First, determine what percentage of the data has a missing value for `revenue`. `r round(pct_miss(df_jobs$revenue), 0)` is a significant proportion of the observations. Are there any apparent patterns to the missingness or does it seem to be random?

```{r}
df_jobs |> 
  ggplot(aes(x=size, y=revenue)) +
  geom_miss_point() +
  theme(
    axis.text.x = element_text(
      angle = 90
    )
  ) +
  labs(
    x = element_blank()
  )
```

This chart shows that most of the missingness occurs for smaller organisations and that when the `size` feature is missing from an observation, the `revenue` feature is usually missing as well.

```{r}
df_jobs |> 
  filter(is.na(size)) |> 
  group_by(revenue) |> 
  count()
  
```

```{r}
df_jobs |> 
  ggplot(aes(x=type_of_ownership, y=revenue)) +
  geom_miss_point() +
  theme(
    axis.text.x = element_text(
      angle = 90
    )
  ) +
  coord_flip() +
  labs(
    x = element_blank(),
    y = element_blank()
  )
```

These two plots give some indication of patterns in the missingness. Smaller organisations appear to have a higher probability of having missing data and the type of ownership also appears important.

Exploring `size` vs `sector` also indicates that

```{r}
df_jobs |> 
  ggplot(aes(x=size, y=sector)) +
  geom_miss_point() +
  theme(
    axis.text.x = element_text(
      angle = 90
    )
  ) +
  labs(
    x = element_blank(),
    y = element_blank()
  )
```

There are

```{r}
df_jobs |> 
  filter(!is.na(size) & is.na(sector)) |> 
  group_by(size) |> 
  count()
```

## Creating factors

Now that the content of `size` and `revenue` have been abbreviated, we can convert these features and others from character to factors.

```{r creating factors}
df_jobs <- df_jobs |>
  mutate(
    # Convert all of the NA values in the factor features to "Unknown"
    across(c(size,type_of_ownership,industry,sector,revenue), ~ str_replace_na(., replacement="Unknown") ),
    # make industry, sector, any type_of_ownership nominal factors
    industry = as_factor(industry),
    sector = as_factor(sector),
    type_of_ownership = as_factor(type_of_ownership),
    # make size and revenue ordinal factors
    size = fct_relevel(size, 
                       "1 to 50",
                       "51 to 200",
                       "201 to 500",
                       "501 to 1000",
                       "1001 to 5000",
                       "5001 to 10000",
                       "10000+"),
    revenue = fct_relevel(revenue, 
                       "Less than $1M",
                       "$1 to $5M",
                       "$5 to $10M",
                       "$10 to $25M",
                       "$25 to $50M",
                       "$50 to $100M",
                       "$100 to $500M",
                       "$500M to $1B",
                       "$1 to $2B",
                       "$2 to $5B",
                       "$5 to $10B",
                       "$10+B")
  )
```


## Creating additional features

Having a salary range for a given job is useful, but plotting ranges in large numbers will be cumbersome. To get around this problem, a salary mid-point feature, `salary_mid_point` will be generated for each job posting. In addition, to make analysis easier, the location feature will be split into two new features, `city` and `state`. This will allow for `group by` statements to be written that will allow for regional analyses to be made.

```{r creating salary city and state features}
df_jobs <- df_jobs |>
  # extract the salary range into two new variables
  extract(salary_estimate, 
          c("salary_lower_end","salary_upper_end"), 
          "\\$(\\d{2,3})K-\\$(\\d{2,3})"
  ) |> 
  # convert the new fields to numeric values
  mutate(
    salary_lower_end = as.numeric(salary_lower_end),
    salary_upper_end = as.numeric(salary_upper_end),
    salary_mid_point = round((salary_lower_end + salary_upper_end)/2,1)
  ) |> 
  extract(
    location,
    c("city","state"), 
    "^(.+),\\s([A-Z]{2})$"
  )
```

### Creating job attributes

A review of the job titles of the postings shows that not all of the positions are for data analysts. The job postings will be filtered to keep postings that meet the following rules:

* if `Data Analyst` (ignore case) is found in `job_title`
* if `Data/(another term) Analyst` (ignore case) is found in `job_title`

```{r}
df_jobs |> 
  distinct(job_title)
```

```{r}
job_filter <- c("Data Analyst", "Data\\s?\\/\\s?\\w+\\sAnalyst")

df_jobs <- df_jobs |> 
  filter(str_detect(job_title, paste(job_filter, collapse = '|')))
```

This reduces the number of job postings to 1661, which is 73.7% of the original postings.

Now that we have eliminated job postings that are likely not data analyst positions, we can focus on determining data analyst tools are included in each posting. To do this, a set of logical features will be added, one for each tool. The tools listed are ones that are commonly used by data analysts.

```{r create technology features}
df_jobs <- df_jobs |>
  mutate(
    has_sql = if_else(str_detect(job_description, "\\bSQL\\b"), TRUE, FALSE),
    has_R = if_else(str_detect(job_description, "\\bR\\b"), TRUE, FALSE),
    has_python = if_else(str_detect(job_description, "\\b[P|p]ython\\b"), TRUE, FALSE),
    has_tableau = if_else(str_detect(job_description, "\\b[T|t]ableau\\b"), TRUE, FALSE),
    has_excel = if_else(str_detect(job_description, "\\b[E|e]xcel\\b"), TRUE, FALSE),
    has_spark = if_else(str_detect(job_description, "\\b[S|s]park\\b"), TRUE, FALSE),
    has_powerbi = if_else(str_detect(job_description, "\\b[P|p]ower\\s?BI\\b"), TRUE, FALSE),
    has_sheets = if_else(str_detect(job_description, "\\b[G|g]oogle [S|s]heets\\b"), TRUE, FALSE)
  )
```

### Education requirements

It is not uncommon for a position to require post secondary or even post graduate education. The data do not contain explicit features that indicated the required education level of a position, but this information is listed in some of the job postings. We can engineer a new feature that indicates the level of education required for a position with a factor. The levels of the factor will be: `Bachelor's`, `Master's`, and `No specified`. We will perform a simple keyword search of the `job_description` feature to try to identify education requirements in job postings. A more sophisticated approach would be to use a large language model to analyse job descriptions, but that is beyond the scope of this analysis.

```{r create education feature}
df_jobs <- df_jobs |>
  mutate(
    education = fct_rev(
      as_factor(
        case_when(
          str_detect(job_description, "Bachelor") ~ "Bachelor's",
          # We don't want to over select on Master so be specific
          str_detect(job_description, "Masters") ~ "Master's", 
          str_detect(job_description, "Master's") ~ "Master's",
          .default = "No specified"
        )
      )
    )
  )
```

## Univariate analysis

Now that the data have been cleaned and new features created, an exploration of univariate data will be done on key features in the dataset. The goal is to understand how the data are distributed, if there are consistency issues, typos, or outliers that need to be investigated.

### `salary_mid_point`

```{r}
df_jobs |> 
  ggplot(aes(x=salary_mid_point)) +
  geom_histogram(binwidth = 10, colour="white", fill=default_fill_colour) +
  labs(
    x="Salary mid point",
    y="Count",
    title="Distribution of job posting salary"
  )
```
While the data isn't normally distributed, it is pretty close. There is a positive skew to the data. That is not surprising.

### `state`

The plot of job postings by state does not reveal any issues with the factor level. The state codes all look correct. The distribution of jobs does raise some questions. While it is not surprising that both California and Texas have a lot of job postings, the very small number in states like Georgia is surprising. More research would need to be conducted to determine why this might be the case. The source of the data does not indicate the methods for collecting the data.

```{r}
df_jobs |> 
  ggplot(aes(x=fct_rev(fct_infreq(state)))) +
    geom_bar(fill=default_fill_colour) +
    coord_flip() +
    labs(
      x=element_blank(),
      y="Job count",
      title="California, Texas, and New York lead in data analyst postings",
      caption="Source: Glassdoor.com"
    )
```

### `size`

While about 10% of the job postings do not disclose the size of the organisation, the number of job postings for each organisation size range is reasonably consistent. Both large and small organisations are looking to fill data analyst positions with no one size of organisation being dominant. This suggests that organisations of all sizes believe that data analysts are important to their success.

```{r}
df_jobs |> 
  filter(!is.na(size)) |> 
  ggplot(aes(x=fct_rev(size))) +
  geom_bar(fill=default_fill_colour) +
  coord_flip() +
  labs(
    x=element_blank(),
    y="Job posting count",
    title="The proportion of jobs is similar across organisation sizes",
    subtitle="Small organisations are well represented"
  )
```

### `type_of_ownership`

The `type_of_ownership` suggests that the data from Glassdoor.com is not a representative sample of the population of data analyst jobs. The data are very heavily drawn from private companies. The table below shows the percentage of jobs by type of ownership, and private companies account for over half of the job postings. Public companies (21%) and `NA` (7.9%) and for the majority of the remaining listings.

```{r}
df_jobs |> 
  filter(!is.na(type_of_ownership)) |> 
  ggplot(aes(x=fct_rev(fct_infreq(type_of_ownership)))) +
  geom_bar(fill=default_fill_colour) +
  coord_flip() +
  labs(
    x=element_blank(),
    y="Job posting count",
    title="Private companies dominate data analyst job postings"
  )
```

```{r}
df_jobs |> 
  group_by(type_of_ownership) |> 
  summarise(
    total = n()
  ) |> 
  mutate(
    pct_total = round(total/sum(total)*100, 1)
  ) |> 
  arrange(desc(pct_total)) |> 
  gt()
```

### `sector`

```{r}
df_jobs |> 
  filter(!is.na(sector)) |> 
  ggplot(aes(x=fct_rev(fct_infreq(sector)))) +
  geom_bar(fill=default_fill_colour) +
  coord_flip() +
  labs(
    x=element_blank(),
    y="Job posting count",
    title="IT and Business Services are leading sectors for data analyst positions",
    caption="Source: Glassdoor.com"
  )
```

There are a few dominant sectors in this dataset and large number of observations that are missing the sector feature. The two leading sectors are Information Technology and Business Services. Finance and Health Care are the next largest categories.

### `rating`

There are 216 job postings that are missing a `rating` value. The feature is close to normally distributed but there are spikes at the extremes. There is a very pronounced spike at 5 and to a much smaller degree at 1. This is likely a result of human psychology. If a company is highly regarded, it seems plausible that instead of rating it a 4, people rate it a 5. The effect seems much smaller when giving a company a negative rating.

```{r}
df_jobs |> 
  filter(!is.na(rating)) |> 
  ggplot(aes(x=rating)) +
  geom_histogram(binwidth = 0.3,colour="white",fill=default_fill_colour) +
  labs(
    x="Company rating",
    y="Count",
    title="Company ratings follow a roughly normal distrbution",
    caption="Source: Glassdoor.com"
  )
```
### `salary_mid_point`

The `salary_mid_point` feature was created from the salary range given in the job posting. It is simply the average of the upper and lower bounds of that range.

```{r}
df_jobs |> 
  ggplot(aes(x=salary_mid_point)) +
  geom_histogram(binwidth = 10,colour="white",fill=default_fill_colour) +
  labs(
    x="Salary midpoint",
    y="Count",
    title="Distribution of salary midpoint for all job postings",
    subtitle="Salaries in thousands of USD",
    caption="Source: Glassdoor.com"
  )
```
### `education`

`education` is a factor with three levels created from the occurrence or absence of the terms "Bachelor" and "Masters/Master's" in the `job_description` feature.

```{r univariate plot of education}
df_jobs |> 
  ggplot(aes(x=education)) +
  geom_bar(fill=default_fill_colour) +
  labs(
    y="Count",
    x="Education level",
    title="Most job postings do no specify a degree requirement"
  )
```
There are no mislabeled observations. The relative lack of Master's level job postings is, perhaps, surprising but could result from the fact that jobs without the "Data Analyst" term have already been filtered out of the dataset. It is possible that these jobs were more likely to be for more senior positions. If required, we could investigate by reviewing the filtered data. That investigation is beyond the scope of this analysis.

## Bivariate analysis

Having looked at univariate data, we will now analyse the relationship between feature pairs of interest.

### `salary_mid_point` vs `rating`

There is only a very weak positive correlation between `rating` and `salary_mid_point`, indicating that compensation isn't a significant factor in an organisation's rating. It also means that choosing an organisation because of how well rated it is will not translate into a higher salary, if there is a linear relationship between `rating` and `salary_mid_point`. There is no obvious trend, linear or nonlinear in the plot below.

```{r}
#| message: false
df_jobs |> 
  filter(!is.na(rating)) |> 
  ggplot(aes(x=rating,y=salary_mid_point)) +
  geom_jitter(alpha=0.4) +
  geom_smooth(method = "lm") +
  labs(
    x="Rating",
    y="Salary midpoint (USD)",
    title="Salary not correlated with company rating"
  )
```
```{r}
#| echo: false
salary_rating_cor <- round(cor(df_jobs$rating, df_jobs$salary_mid_point, use = "complete.obs"),3)
```


The correlation coefficient is `r salary_rating_cor` which is quite small. We can say that there is, at best, a very weak positive correlation between `rating` and `salary_mid_point`.

### `salary_mid_point` vs `size`

Comparing boxplots of `salary_mid_point` vs `size` shows that there is not a significant difference in salaries based on organisation size.

```{r}
df_jobs |> 
  filter(!is.na(size)) |> 
  ggplot(aes(x=size, y=salary_mid_point)) +
  geom_boxplot(linewidth=1) +
  geom_jitter(alpha=0.4) +
  scale_colour_brewer(palette = "Dark2") +
  labs(
    x="Organisation number of employees",
    y="Salary midpoint (USD)",
    caption = "Source: Glassdoor.com",
    title="Organisation size has little impact on salary"
  ) +
  coord_flip() +
  theme(
    legend.position = "none"
  )
```
We can use a general linear model to compare the `size` groups.

```{r}
lm_size_model <- df_jobs |> 
  filter(!is.na(size)) |>
  lm(salary_mid_point ~ size, data=_)

p_value <- compute_p(lm_size_model)

lm_size_model |> 
  summary()
```
This result confirms what we could see in the boxplots. There is no significant difference between the groups. The *p-value* for the test is $p=`r round(p_value, 4)`$ and the estimates for all groups are small and have high *p-values*. 

### `type_of_ownership` vs `salary_mid_point`

As with `size`, there is not a significant difference in salary between groups.

```{r}
df_jobs |> 
  filter(!is.na(type_of_ownership)) |> 
  group_by(type_of_ownership) |> 
  count() |> 
  filter(n > 15) |>  
  inner_join(df_jobs, by="type_of_ownership") |> 
  ggplot(aes(x=type_of_ownership, y=salary_mid_point)) +
  geom_jitter(alpha=0.2) +
  geom_boxplot(linewidth=1,fill="transparent",outlier.shape = NA) +
  scale_colour_brewer(palette = "Dark2") +
  coord_flip() +
  labs(
    x=element_blank(),
    y="Salary midpoint",
    caption = "Source: Glassdoor.com",
    title="Ownership type has little impact on salary",
    subtitle="salaries in USD"
  ) +
  theme(
    legend.position = "none"
  )
```
```{r}
df_jobs |> 
  filter(!is.na(type_of_ownership)) |> 
  group_by(type_of_ownership) |> 
  count() |> 
  filter(n > 15) |>  
  inner_join(df_jobs, by="type_of_ownership") |> 
  lm(salary_mid_point~type_of_ownership,data=_) |> 
  summary()
```
The is one interesting result here, that the Government group is significantly different to the others. The box plots hint at this, and the linear model confirms it at the 95% confidence level.

### `education` vs `salary_mid_point`

```{r}
df_jobs |> 
  ggplot(aes(x=education, y=salary_mid_point)) +
    geom_jitter(alpha=0.4)
```
```{r}
df_jobs |> 
  lm(salary_mid_point~education, data=_) |> 
  summary()
```
This is a surprising finding. The plotted data *and* the model suggest that job postings that marked as requiring a master's degree make **less** money than those that require only a bachelor's or don't specify. That seems unlikely.

Perhaps the `sector` a job posting is listed in would be instructive.

```{r}
df_jobs |> 
  filter(education == "Master's") |> 
  group_by(sector) |> 
  summarise(
    total = n(),
    med_sal = median(salary_mid_point)
  )
```
This reveals that there is a significant group imbalance in the data. The vast majority of the jobs are listed under Information Technology and so that group dominates. That group also has the second lowest median salary of the 9 groups in the dataset.

After reviewing a number of the job descriptions, it is clear that there is a lot of variability in what is being requested. Some postings simply "prefer" a Master's degree while other require it. Furthermore, the type of Master's degree being requested varies. Some require Statistics/Applied Mathematics, some Compute Science, and other Master's degrees in social sciences.

Given these findings, this feature, as constructed, does not appear to offer any insights into the data. A simple keyword search of the `job_description` field is inadequate to capture the variability of the data in a meaningful way.

## Popular tools and technologies

An analysis of technology keyword occurrences shows that SQL is the most frequently requested technology skill. SQL appears in 64.8% of all job postings. Other popular skills are Excel, Python, Tableau, and R.

```{r}
df_da_tools <- df_jobs |> 
  summarise(
    SQL=compute_percentage(has_sql, n()),
    R=compute_percentage(has_R, n()),
    Python=compute_percentage(has_python, n()),
    Excel=compute_percentage(has_excel, n()),
    Tableau=compute_percentage(has_tableau, n()),
    Spark=compute_percentage(has_spark, n()),
    PowerBI=compute_percentage(has_powerbi, n()),
    Sheets=compute_percentage(has_sheets, n())
  ) |> 
  pivot_longer(cols = everything(), names_to = "technology", values_to = "pct_of_jobs")

df_da_tools |> 
  mutate(
    technology = fct_reorder(technology, pct_of_jobs)
  ) |> 
  ggplot(aes(x=technology, y=pct_of_jobs)) +
  geom_col(fill=default_fill_colour) +
  coord_flip() +
  labs(
    x=element_blank(),
    y="Percent of all job postings",
    title="SQL & Excel are the most requested skills",
    subtitle="Popularity of standard data analyst tools"
  )
```

### Do jobs requiring SQL pay more?

Do job postings requiring SQL have a higher salary than those that don't? The data do not support the idea. Job posting that list SQL as a desired skill have a very similar median to those that do not. The salary range is tighter for jobs that include SQL, but there is clearly no salary advantage. There are, however, far more jobs that list SQL than do not, so it is clearly an important skill.

```{r}
df_jobs |> 
  ggplot(aes(x=has_sql,y=salary_mid_point)) +
  geom_jitter(alpha=0.1) +
  geom_boxplot(linewidth=0.75, fill="transparent", outlier.shape = NA) +
  labs(
    x="Includes SQL",
    y="Salary midpoint",
    title="Comparison of job posting salaries",
    subtitle="Salaries (USD) for jobs requiring SQL"
  )
```

### Does R or Python command more pay?

The two primary coding languages used by data analysts are Python and R. These skills are requested in 497 and 351 job listings, respectively. Both languages are included in 288 job listings. Python is the more popular of the two in this dataset. One potential issue here is the bias in this dataset to private companies. R is particularly popular in the medical space and in academia. If those sectors are underrepresented in this dataset, then the results of this comparison may be misleading. 

```{r}
df_jobs |> 
  summarise(
    R = sum(has_R),
    Python = sum(has_python),
    Both = sum(has_R&has_python)
  ) |> 
  pivot_longer(cols = everything(), names_to = "Language", values_to = "Jobs")
```

Filtering out the job postings that list both languages, the salaries of R-only and Python-only jobs can be compared.

```{r}
df_jobs |> 
  filter(xor(has_python,has_R)) |> 
  mutate(
    coding_lang = if_else(has_python, "Python", "R")
  ) |> 
  ggplot(aes(x=coding_lang,y=salary_mid_point)) +
  geom_jitter(alpha=0.1) +
  geom_boxplot(linewidth=0.75, fill="transparent", outlier.shape = NA) +
  labs(
    x="Coding language",
    y="Salary midpoint",
    title="Comparison of job posting salaries",
    subtitle="Salaries (USD) for jobs requiring R or Python"
  )
```
The Python-only jobs have a noticeably higher mean salary and a wider range of salaries in general. The mean for Python-only jobs is about \$75.8K `r paste("$", 80.0, "K", sep="")` while for R-only jobs the mean is about \$65.7K

```{r}
df_jobs |> 
  filter(xor(has_python,has_R)) |> 
  mutate(
    coding_lang = if_else(has_python, "Python", "R")
  ) |> 
  lm(salary_mid_point ~ coding_lang, data=_) |> 
  summary()
```

## Findings


## Summary and recommendations




